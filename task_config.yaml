task:
  # The name of the task
  task_name: weibo
  # The input data directory
  data_dir: data/weibo
  # bert pre-trained model directory
  bert_model_dir: bert-pretrained/chinese_L-12_H-768_A-12
  # The output directory where the model predictions and checkpoints will be written
  output_dir: pre-trained-model/model_weibo/origin_model/
  log_dir: log/
  # Specify the ckeckpoint to load
  checkpoint: ~
  # Whether you are using an uncased model or not
  lower_case: true
  # The maximum total input sequence length after WordPiece tokenization
  max_seq_length: 180
  # Whether the dataset is standard
  standard: true
train:
  # Whether do training in this run
  do: true
  # Total batch size for training
  batch_size: 25
  # The initial learning rate for Adam
  learning_rate: !!float 2e-5
  # Total number of training epochs to perform
  epochs: 100
  # Proportion of training to perform linear learning rate warmup for
  warmup_proportion: 0.1
  # Number of updates steps to accumulate before performing a backward/update pass
  gradient_accumulation_steps: 1
  # Random seed for initialization
  seed: 33
predict:
  # Whether do predicting in this run
  do: true
  # Which dataset to be predicted: train, dev or test
  dataset: test
  # Total batch size for predicting
  batch_size: 25
# Whether to use CUDA when available
use_cuda: true
